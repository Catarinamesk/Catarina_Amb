{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Catarinamesk/Catarina_Amb/blob/main/Estruturas_de_Dados_DataOps_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ed93c06",
      "metadata": {
        "id": "3ed93c06"
      },
      "source": [
        "# Exercício Prático: Estruturas de Dados no DataOps\n",
        "Este notebook irá explorar como diferentes formatos de dados (CSV, JSON, Parquet) são utilizados no contexto de DataOps. Vamos carregar, transformar e comparar os formatos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdb3b5be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdb3b5be",
        "outputId": "7c8f7658-a306-45a2-fb90-14c500153c97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Carregando o arquivo CSV...\n",
            "Pré-visualização dos dados:\n",
            "   TransactionID  CustomerID  ProductID ProductName Category  Quantity  \\\n",
            "0              1        1185        103      others   simple         8   \n",
            "1              2        4819        136       force     sell         2   \n",
            "2              3        4297        140        deal    serve         4   \n",
            "3              4        4365        144       later  another         6   \n",
            "4              5        2408        176         end     baby         9   \n",
            "\n",
            "   PricePerUnit  TotalPrice PurchaseDate Region  PaymentMethod  \n",
            "0        750.01     6000.08   2025-01-07  South    Credit Card  \n",
            "1        515.56     1031.12   2025-01-06   West  Bank Transfer  \n",
            "2        980.50     3922.00   2025-01-01  North    Credit Card  \n",
            "3        507.34     3044.04   2025-01-06  North         PayPal  \n",
            "4        300.11     2700.99   2025-01-23  South         PayPal  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Importar bibliotecas necessárias\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 1. Carregar o dataset em formato CSV\n",
        "# O formato CSV é amplamente usado por ser simples e compatível com a maioria das ferramentas.\n",
        "# Aqui carregamos o arquivo \"sales.csv\" para iniciar o exercício.\n",
        "print(\"Carregando o arquivo CSV...\")\n",
        "df = pd.read_csv(\"sales.csv\")\n",
        "print(\"Pré-visualização dos dados:\")\n",
        "print(df.head())  # Mostra as primeiras 5 linhas do dataset para inspeção inicial.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9b96242",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9b96242",
        "outputId": "ad5e4e87-0231-4c3d-c2f8-55cf513c2583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Salvando o arquivo em formato JSON...\n",
            "Arquivo JSON salvo como 'sales.json'.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 2. Salvar os dados em formato JSON\n",
        "# O JSON é ideal para integração com APIs e sistemas web. Usamos \"orient='records'\" para criar\n",
        "# um JSON estruturado linha a linha, onde cada registro (linha) do DataFrame vira um objeto JSON.\n",
        "# \"lines=True\" é necessário para salvar os registros como linhas separadas (JSONL), facilitando o streaming.\n",
        "print(\"\\nSalvando o arquivo em formato JSON...\")\n",
        "df.to_json(\"sales.json\", orient=\"records\", lines=True)\n",
        "print(\"Arquivo JSON salvo como 'sales.json'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aca7be9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aca7be9f",
        "outputId": "0ea34183-e8b5-42cd-a9b6-e5ef0897bcc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Salvando o arquivo em formato Parquet...\n",
            "Arquivo Parquet salvo como 'sales.parquet'.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3. Salvar os dados em formato Parquet\n",
        "# O formato Parquet é escolhido por sua eficiência em compactação e leitura de dados em colunas.\n",
        "# Usamos \"engine='pyarrow'\" por ser uma biblioteca moderna e otimizada para manipulação de Parquet.\n",
        "# O Parquet é amplamente usado em plataformas de Big Data (como Spark e Hadoop).\n",
        "print(\"\\nSalvando o arquivo em formato Parquet...\")\n",
        "df.to_parquet(\"sales.parquet\", engine=\"pyarrow\")\n",
        "print(\"Arquivo Parquet salvo como 'sales.parquet'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76C41GyLk77F",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76C41GyLk77F",
        "outputId": "0b99c65b-1631-4a64-9e03-58be2e6797fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'PAR1\\x15\\x04\\x15\\x80\\xe2\\t\\x15\\xd2\\xf1\\x04L\\x15\\xa0\\x9c\\x01\\x15\\x00\\x12\\x00\\x00\\x80\\xf1\\x04\\x04\\x01\\x00\\t\\x01\\x00\\x02\\t\\x07\\x04\\x00\\x03\\r\\x08\\x00\\x04\\r\\x08\\x00\\x05\\r\\x08\\x00\\x06\\r\\x08\\x00\\x07\\r\\x08\\x00\\x08\\r\\x08\\x00\\t\\r\\x08\\x00\\n\\r\\x08\\x00\\x0b\\r\\x08\\x00\\x0c\\r\\x08\\x00\\r\\r\\x08\\x00\\x0e\\r\\x08\\x00\\x0f\\r\\x08\\x00\\x10\\r\\x08\\x00\\x11\\r\\x08\\x00\\x12\\r\\x08\\x00\\x13\\r\\x08\\x00\\x14\\r\\x08\\x00\\x15\\r\\x08\\x00\\x16\\r\\x08\\x00\\x17\\r\\x08\\x00\\x18\\r\\x08\\x00\\x19\\r\\x08\\x00\\x1a\\r\\x08\\x00\\x1b\\r\\x08\\x00\\x1c\\r\\x08\\x00\\x1d\\r\\x08\\x00\\x1e\\r\\x08\\x00\\x1f\\r\\x08\\x00 \\r\\x08\\x00!\\r\\x08\\x00\"\\r\\x08\\x00#\\r\\x08\\x00$\\r\\x08\\x00%\\r\\x08\\x00&\\r\\x08\\x00\\'\\r\\x08\\x00(\\r\\x08\\x00)\\r\\x08\\x00*\\r\\x08\\x00+\\r\\x08\\x00,\\r\\x08\\x00-\\r\\x08\\x00.\\r\\x08\\x00/\\r\\x08\\x000\\r\\x08\\x001\\r\\x08\\x002\\r\\x08\\x003\\r\\x08\\x004\\r\\x08\\x005\\r\\x08\\x006\\r\\x08\\x007\\r\\x08\\x008\\r\\x08\\x009\\r\\x08\\x00:\\r\\x08\\x00;\\r\\x08\\x00<\\r\\x08\\x00=\\r\\x08\\x00>\\r\\x08\\x00?\\r\\x08\\x00@\\r\\x08\\x00A\\r\\x08\\x00B\\r\\x08\\x00C\\r\\x08\\x00D\\r'\n"
          ]
        }
      ],
      "source": [
        "# Abrir o arquivo Parquet em modo binário\n",
        "with open(\"sales.parquet\", \"rb\") as f:\n",
        "    content = f.read(300)  # Ler os primeiros 300 bytes\n",
        "    print(content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LkU82PwflDbE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkU82PwflDbE",
        "outputId": "7ad20bcf-26af-487a-c4d5-6bb6b142a87a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Esquema do arquivo Parquet:\n",
            "<pyarrow._parquet.ParquetSchema object at 0x15e9c6c40>\n",
            "required group field_id=-1 schema {\n",
            "  optional int64 field_id=-1 TransactionID;\n",
            "  optional int64 field_id=-1 CustomerID;\n",
            "  optional int64 field_id=-1 ProductID;\n",
            "  optional binary field_id=-1 ProductName (String);\n",
            "  optional binary field_id=-1 Category (String);\n",
            "  optional int64 field_id=-1 Quantity;\n",
            "  optional double field_id=-1 PricePerUnit;\n",
            "  optional double field_id=-1 TotalPrice;\n",
            "  optional binary field_id=-1 PurchaseDate (String);\n",
            "  optional binary field_id=-1 Region (String);\n",
            "  optional binary field_id=-1 PaymentMethod (String);\n",
            "}\n",
            "\n",
            "\n",
            "Informações gerais:\n",
            "Número de linhas: 10000\n",
            "Número de colunas: 11\n",
            "\n",
            "Metadados completos:\n",
            "<pyarrow._parquet.FileMetaData object at 0x15e9b7830>\n",
            "  created_by: parquet-cpp-arrow version 16.1.0\n",
            "  num_columns: 11\n",
            "  num_rows: 10000\n",
            "  num_row_groups: 1\n",
            "  format_version: 2.6\n",
            "  serialized_size: 6254\n"
          ]
        }
      ],
      "source": [
        "import pyarrow.parquet as pq\n",
        "\n",
        "# Ler o arquivo Parquet\n",
        "parquet_file = pq.ParquetFile(\"sales.parquet\")\n",
        "\n",
        "# Mostrar os metadados do Parquet\n",
        "print(\"Esquema do arquivo Parquet:\")\n",
        "print(parquet_file.schema)\n",
        "\n",
        "# Mostrar o número de linhas e colunas\n",
        "print(\"\\nInformações gerais:\")\n",
        "print(f\"Número de linhas: {parquet_file.metadata.num_rows}\")\n",
        "print(f\"Número de colunas: {parquet_file.metadata.num_columns}\")\n",
        "\n",
        "# Mostrar os metadados completos\n",
        "print(\"\\nMetadados completos:\")\n",
        "print(parquet_file.metadata)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZBXYNlbylF44",
      "metadata": {
        "id": "ZBXYNlbylF44"
      },
      "source": [
        "* O Parquet é um formato binário eficiente, mas não legível diretamente por humanos.\n",
        "* Ferramentas como extensões no VSCode convertem os metadados e dados em estruturas JSON-like para facilitar a visualização.\n",
        "* Para manipulação real, usamos bibliotecas como pandas ou ferramentas de Big Data (Spark, Hive).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d407809e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d407809e",
        "outputId": "5b9057db-dbc0-4755-b2b4-012faae69d29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Comparando tamanhos dos arquivos:\n",
            "Tamanho do arquivo CSV: 708.44 KB\n",
            "Tamanho do arquivo JSON: 2224.79 KB\n",
            "Tamanho do arquivo Parquet: 285.64 KB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 4. Comparar os tamanhos dos arquivos gerados\n",
        "# Aqui usamos a biblioteca os para verificar os tamanhos dos arquivos gerados.\n",
        "# A função os.path.getsize retorna o tamanho em bytes, que convertemos para KB (dividindo por 1024).\n",
        "print(\"\\nComparando tamanhos dos arquivos:\")\n",
        "csv_size = os.path.getsize(\"sales.csv\") / 1024  # Tamanho do arquivo CSV\n",
        "json_size = os.path.getsize(\"sales.json\") / 1024  # Tamanho do arquivo JSON\n",
        "parquet_size = os.path.getsize(\"sales.parquet\") / 1024  # Tamanho do arquivo Parquet\n",
        "\n",
        "# Exibindo os tamanhos para análise\n",
        "print(f\"Tamanho do arquivo CSV: {csv_size:.2f} KB\")\n",
        "print(f\"Tamanho do arquivo JSON: {json_size:.2f} KB\")\n",
        "print(f\"Tamanho do arquivo Parquet: {parquet_size:.2f} KB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "866b3897",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "866b3897",
        "outputId": "8d5fc984-8de9-46fc-a35e-fcb7d859e378"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Estrutura do arquivo JSON:\n",
            "{\"TransactionID\":1,\"CustomerID\":1185,\"ProductID\":103,\"ProductName\":\"others\",\"Category\":\"simple\",\"Quantity\":8,\"PricePerUnit\":750.01,\"TotalPrice\":6000.08,\"PurchaseDate\":\"2025-01-07\",\"Region\":\"South\",\"PaymentMethod\":\"Credit Card\"}\n",
            "{\"TransactionID\":2,\"CustomerID\":4819,\"ProductID\":136,\"ProductName\":\"forc\n",
            "\n",
            "Estrutura do arquivo Parquet:\n",
            "   TransactionID  CustomerID  ProductID ProductName Category  Quantity  \\\n",
            "0              1        1185        103      others   simple         8   \n",
            "1              2        4819        136       force     sell         2   \n",
            "2              3        4297        140        deal    serve         4   \n",
            "3              4        4365        144       later  another         6   \n",
            "4              5        2408        176         end     baby         9   \n",
            "\n",
            "   PricePerUnit  TotalPrice PurchaseDate Region  PaymentMethod  \n",
            "0        750.01     6000.08   2025-01-07  South    Credit Card  \n",
            "1        515.56     1031.12   2025-01-06   West  Bank Transfer  \n",
            "2        980.50     3922.00   2025-01-01  North    Credit Card  \n",
            "3        507.34     3044.04   2025-01-06  North         PayPal  \n",
            "4        300.11     2700.99   2025-01-23  South         PayPal  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 5. Visualizar a estrutura de cada formato\n",
        "# JSON: Exibimos os primeiros 300 caracteres do arquivo para que os alunos analisem a estrutura.\n",
        "print(\"\\nEstrutura do arquivo JSON:\")\n",
        "with open(\"sales.json\", \"r\") as f:\n",
        "    print(f.read(300))  # Visualiza os primeiros 300 caracteres para inspecionar o conteúdo\n",
        "\n",
        "# Parquet: Reabrimos o arquivo usando pandas para verificar se os dados foram salvos corretamente.\n",
        "print(\"\\nEstrutura do arquivo Parquet:\")\n",
        "parquet_df = pd.read_parquet(\"sales.parquet\")  # Lendo o arquivo Parquet como DataFrame\n",
        "print(parquet_df.head())  # Mostra as primeiras 5 linhas para validar a integridade dos dados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XDBjiMc9hbvY",
      "metadata": {
        "id": "XDBjiMc9hbvY"
      },
      "source": [
        "Reflexão:\n",
        "- O formato JSON é mais legível e ideal para APIs, mas ocupa mais espaço.\n",
        "- O formato Parquet é eficiente para armazenamento e processamento em escala.\n",
        "- O formato CSV é simples e universal, mas pouco eficiente em compactação."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-h1R3sWdjlXc",
      "metadata": {
        "id": "-h1R3sWdjlXc"
      },
      "source": [
        "\n",
        "# Análise dos Tamanhos de Arquivos\n",
        "\n",
        "### Resultados Obtidos:\n",
        "- **CSV:** 708.44 KB\n",
        "- **JSON:** 2224.79 KB\n",
        "- **Parquet:** 285.67 KB\n",
        "\n",
        "---\n",
        "\n",
        "## **Análise dos Tamanhos:**\n",
        "\n",
        "### 1. **CSV (708.44 KB):**\n",
        "- O **CSV** é simples, sem compactação, e armazena os dados como texto puro.  \n",
        "- É mais eficiente em termos de tamanho que JSON, mas ainda não ideal para grandes volumes.\n",
        "\n",
        "### 2. **JSON (2224.79 KB):**\n",
        "- O **JSON** é mais flexível e descritivo, mas isso vem com um custo:  \n",
        "  - Os nomes das chaves (colunas) são repetidos em cada linha.  \n",
        "  - Por isso, ocupa significativamente mais espaço, especialmente em datasets grandes.  \n",
        "- Apesar disso, é o melhor para integração com APIs e sistemas.\n",
        "\n",
        "### 3. **Parquet (285.67 KB):**\n",
        "- O **Parquet** mostra sua força:  \n",
        "  - Compactação altamente eficiente, reduzindo o tamanho em **~60%** comparado ao CSV.  \n",
        "  - É otimizado para leitura em colunas, o que o torna ideal para análises em escala no Big Data.  \n",
        "  - Um ótimo formato para armazenamento e processamento distribuído.\n",
        "\n",
        "---\n",
        "\n",
        "## **Conclusão Prática:**\n",
        "\n",
        "- **Parquet é a escolha certa** para pipelines de **Big Data e DataOps** quando a eficiência é uma prioridade.  \n",
        "- O **JSON**, embora maior, é indispensável para sistemas que precisam de dados legíveis e estruturados.  \n",
        "- O **CSV** continua sendo uma boa opção para ingestão inicial e compatibilidade ampla.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}